# -*- coding: utf-8 -*-
"""DSN_blending.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S_DKY57N4fJszHTF7ewAbhKHm2jD9vZ_
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np  #linear algebra
import pandas as pd  #data processing
import matplotlib.pyplot as plt #plots
# %matplotlib inline
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

tr = pd.read_csv('/content/drive/My Drive/DSC AI Stack /bootcamp comprtition/Train (1).csv')
te = pd.read_csv('/content/drive/My Drive/DSC AI Stack /bootcamp comprtition/Test (1).csv')
ss = pd.read_csv('/content/drive/My Drive/DSC AI Stack /bootcamp comprtition/SampleSubmission.csv')

train = tr.copy()
test = te.copy()

!pip install catboost

pd.set_option('display.max_columns',None)
pd.set_option('display.max_rows',None)

train.head()

train.isnull().sum()

train.describe()

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
train['form_field47'] = le.fit_transform(train.form_field47)
test['form_field47'] = le.fit_transform(test.form_field47)
train['default_status'] = le.fit_transform(train.default_status)

features_with_integer_values = ['form_field16','form_field17','form_field18',
                                'form_field19','form_field20','form_field34','form_field35',
                                'form_field36','form_field37','form_field38','form_field39','form_field45','form_field46']

features_recorded_in_days = ['form_field26','form_field27','form_field28','form_field29','form_field30','form_field31']

features_recorded_in_months = ['form_field32']

categorical_features = ['form_field47']

#All other features are of float type

mean_fill = features_with_integer_values + features_recorded_in_days + features_recorded_in_months
#mean_fill

for i in mean_fill:
    train[i].fillna(train[i].mean(), inplace = True)    
    test[i].fillna(test[i].mean(), inplace = True)

cont_feat = ['form_field1','form_field2','form_field3','form_field4','form_field5','form_field6','form_field7','form_field8',
            'form_field9','form_field10','form_field11','form_field12','form_field13','form_field15','form_field21',
             'form_field22','form_field23','form_field24','form_field25','form_field33', 'form_field40','form_field41',
            'form_field42','form_field43','form_field44','form_field48','form_field49','form_field50']

for i in cont_feat:
    train[i].fillna(-1, inplace = True)    
    test[i].fillna(-1, inplace = True)

from sklearn.preprocessing import StandardScaler

train.drop('Applicant_ID', axis=1, inplace= True)
test.drop('Applicant_ID', axis=1, inplace= True)

#instantiate Standard scaler
sc = StandardScaler()

#scale the data... scaling is done to center the data distribution around zero and standardize its standard deviation to 1
scaled_train = pd.DataFrame(sc.fit_transform(train), columns = train.columns)
scaled_test = pd.DataFrame(sc.fit_transform(test), columns = test.columns)

X = train.drop('default_status', axis = 1)
y = train['default_status']

from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier     # algorithm for training and testing
from xgboost import XGBClassifier                       # extreme boosting algorithm
from lightgbm import LGBMClassifier                               # extreme boosting algorithm
from catboost import CatBoostClassifier
from sklearn.metrics import roc_curve, auc, roc_auc_score

lr = LogisticRegression()
rfc = RandomForestClassifier(n_estimators=400, random_state=42, n_jobs= -1) 
etc = ExtraTreesClassifier()
xgb = XGBClassifier(n_estimators=300, random_state=42, scale_pos_weight=3.08, min_sample_split = 9, min_sample_leaf = 2, max_features = 'auto')
lgb = LGBMClassifier(reg_lambda = 2, random_state = 42, n_estimators = 4000)
cb = CatBoostClassifier(n_estimators=4000,eval_metric='AUC',learning_rate=0.1, max_depth=4, reg_lambda =2,
                       bootstrap_type='Bayesian', use_best_model=True,od_wait=50,random_state = 42)

n=10
skf = StratifiedKFold(n_splits = n, shuffle = True, random_state=42)

scores_cb = []
pred_test1 = 0
for train_index, test_index in skf.split(X,y):
    print('train index: ', train_index, '\n')
    print('test index: ', test_index, '\n')
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    cb.fit(X_train, y_train,eval_set=[(X_train, y_train),(X_test, y_test)],verbose=100, early_stopping_rounds=120)
    pred = cb.predict_proba(X_test)[:,1]
    print('score: ', roc_auc_score(y_test, pred))
    scores_cb.append(roc_auc_score(y_test, pred))
    test_pred1 = cb.predict_proba(test)[:,1]
    pred_test1 += test_pred1

np.mean(scores_cb)

pred_test1

scores_cb

final_pred1 = pred_test1/n
ss['default_status'] = final_pred1
ss.to_csv('cb2_blend_submission.csv', index = False)

scores_xgb = []
pred_test2 = 0
for train_index, test_index in skf.split(X,y):
    print('train index: ', train_index, '\n')
    print('test index: ', test_index, '\n')
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    xgb.fit(X_train, y_train,eval_set=[(X_train, y_train),(X_test, y_test)],verbose=100, early_stopping_rounds=120)
    pred = xgb.predict_proba(X_test)[:,1]
    print('score: ', roc_auc_score(y_test, pred))
    scores_xgb.append(roc_auc_score(y_test, pred))
    test_pred2 = xgb.predict_proba(test)[:,1]
    pred_test2 += test_pred2

np.mean(scores_xgb)

final_pred2 = pred_test2/n
ss['default_status'] = final_pred2
ss.to_csv('xgb2_blend_submission.csv', index = False)

scores_lgb = []
pred_test3 = 0
for train_index, test_index in skf.split(X,y):
    print('train index: ', train_index, '\n')
    print('test index: ', test_index, '\n')
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    lgb.fit(X_train, y_train,eval_set=[(X_train, y_train),(X_test, y_test)],verbose=100, early_stopping_rounds=120)
    pred = lgb.predict_proba(X_test)[:,1]
    print('score: ', roc_auc_score(y_test, pred))
    scores_lgb.append(roc_auc_score(y_test, pred))
    test_pred3 = lgb.predict_proba(test)[:,1]
    pred_test3 += test_pred3

np.mean(scores_lgb)

final_pred3 = pred_test3/n
ss['default_status'] = final_pred1
ss.to_csv('lgb2_blend_submission.csv', index = False)

scores_rfc = []
pred_test4 = 0
for train_index, test_index in skf.split(X,y):
    print('train index: ', train_index, '\n')
    print('test index: ', test_index, '\n')
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    rfc.fit(X_train, y_train)
    pred = rfc.predict_proba(X_test)[:,1]
    print('score: ', roc_auc_score(y_test, pred))
    scores_rfc.append(roc_auc_score(y_test, pred))
    test_pred4 = rfc.predict_proba(test)[:,1]
    pred_test4 += test_pred4

np.mean(scores_rfc)

final_pred4 = pred_test4/n
ss['default_status'] = final_pred1
ss.to_csv('rfc2_blend_submission.csv', index = False)

cb2 = pd.read_csv('/content/cb2_blend_submission.csv')
xbg2 = pd.read_csv('/content/xgb2_blend_submission.csv')
lgb2 = pd.read_csv('/content/lgb2_blend_submission.csv')
rfc2 = pd.read_csv('/content/rfc2_blend_submission.csv')

final_blend2 = ((cb2.default_status * 0.8 + xbg2.default_status * 0.2) + 
                (lgb2.default_status * 0.9 + rfc2.default_status * 0.1))
ss['default_status'] = final_blend2
ss.to_csv('final2_blend2_submission.csv', index = False)

final_blend2 = ((cb2.default_status * 0.7 + xbg2.default_status * 0.3) + 
                (lgb2.default_status * 0.8 + rfc2.default_status * 0.2))
ss['default_status'] = final_blend2
ss.to_csv('final_submission.csv', index = False)

